{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONFIDENCE INTERVALS IN APPLIED MACHINE LEARNING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A confidence interval is a bounds on the estimate of a population variable. It is an interval statistic used to quantify the uncertainty on an estimate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  _A confidence interval to contain an unknown characteristic of the population or process. The quantity of interest might be a population property or “parameter”, such as the mean or standard deviation of the population or process._ \n",
    "\n",
    "— Page 3, Statistical Intervals: A Guide for Practitioners and Researchers, 2017."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A confidence interval is different from a tolerance interval that describes the bounds of data sampled from the distribution. It is also different from a prediction interval that describes the bounds on a single observation. Instead, the confidence interval provides bounds on a population parameter, such as a mean, standard deviation, or similar.\n",
    "\n",
    "In applied machine learning, we may wish to use confidence intervals in the presentation of the skill of a predictive model.\n",
    "\n",
    "For example, a confidence interval could be used in presenting the skill of a classification model, which could be stated as:\n",
    "\n",
    "Given the sample, there is a 95% likelihood that the range x to y covers the true model accuracy.\n",
    "\n",
    "or\n",
    "\n",
    "The accuracy of the model was x +/- y at the 95% confidence level.\n",
    "\n",
    "Confidence intervals can also be used in the presentation of the error of a regression predictive model; for example:\n",
    "\n",
    "There is a 95% likelihood that the range x to y covers the true error of the model.\n",
    "\n",
    "or\n",
    "\n",
    "The error of the model was x +/- y at the 95% confidence level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The choice of 95% confidence is very common in presenting confidence intervals, although other less common values are used, such as 90% and 99.7%. In practice, you can use any value you prefer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _The 95% confidence interval (CI) is a range of values calculated from our data, that most likely, includes the true value of what we’re estimating about the population._\n",
    "\n",
    "-- Page 4, Introduction to the New Statistics: Estimation, Open Science, and Beyond, 2016."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value of a confidence interval is its ability to quantify the uncertainty of the estimate. It provides both a lower and upper bound and a likelihood. Taken as a radius measure alone, the confidence interval is often referred to as the margin of error and may be used to graphically depict the uncertainty of an estimate on graphs through the use of error bars.\n",
    "\n",
    "Often, the larger the sample from which the estimate was drawn, the more precise the estimate and the smaller (better) the confidence interval.\n",
    "\n",
    "- Smaller Confidence Interval: A more precise estimate.\n",
    "- Larger Confidence Interval: A less precise estimate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _We can also say that the CI tells us how precise our estimate is likely to be, and the margin of error is our measure of precision. A short CI means a small margin of error and that we have a relatively precise estimate […] A long CI means a large margin of error and that we have a low precision_\n",
    "\n",
    "--  Page 4, Introduction to the New Statistics: Estimation, Open Science, and Beyond, 2016."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confidence intervals belong to a field of statistics called estimation statistics that can be used to present and interpret experimental results instead of, or in addition to, statistical significance tests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _Estimation gives a more informative way to analyze and interpret results. […] Knowing and thinking about the magnitude and precision of an effect is more useful to quantitative science than contemplating the probability of observing data of at least that extremity, assuming absolutely no effect._\n",
    "\n",
    "-- — Estimation statistics should replace significance testing, 2016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confidence intervals may be preferred in practice over the use of statistical significance tests.\n",
    "\n",
    "The reason is that they are easier for practitioners and stakeholders to relate directly to the domain. They can also be interpreted and used to compare machine learning models.\n",
    "\n",
    "_These estimates of uncertainty help in two ways. First, the intervals give the consumers of the model an understanding about how good or bad the model may be. […] In this way, the confidence interval helps gauge the weight of evidence available when comparing models. The second benefit of the confidence intervals is to facilitate trade-offs between models. If the confidence intervals for two models significantly overlap, this is an indication of (statistical) equivalence between the two and might provide a reason to favor the less complex or more interpretable model._\n",
    "\n",
    "— Page 416, Applied Predictive Modeling, 2013.\n",
    "\n",
    "Now that we know what a confidence interval is, let’s look at a few ways that we can calculate them for predictive models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interval for Classification Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification problems are those where a label or class outcome variable is predicted given some input data.\n",
    "\n",
    "It is common to use classification accuracy or classification error (the inverse of accuracy) to describe the skill of a classification predictive model. For example, a model that makes correct predictions of the class outcome variable 75% of the time has a classification accuracy of 75%, calculated as:\n",
    "\n",
    "accuracy = total correct predictions / total predictions made * 100\n",
    "\n",
    "accuracy = total correct predictions / total predictions made * 100\n",
    "\n",
    "This accuracy can be calculated based on a hold-out dataset not seen by the model during training, such as a validation or test dataset.\n",
    "\n",
    "Classification accuracy or classification error is a proportion or a ratio. It describes the proportion of correct or incorrect predictions made by the model. Each prediction is a binary decision that could be correct or incorrect. Technically, this is called a Bernoulli trial, named for Jacob Bernoulli. The proportions in a Bernoulli trial have a specific distribution called a binomial distribution. Thankfully, with large sample sizes (e.g. more than 30), we can approximate the distribution with a Gaussian."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_In statistics, a succession of independent events that either succeed or fail is called a Bernoulli process. […] For large N, the distribution of this random variable approaches the normal distribution._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the assumption of a Gaussian distribution of the proportion (i.e. the classification accuracy or error) to easily calculate the confidence interval.\n",
    "\n",
    "In the case of classification error, the radius of the interval can be calculated as:\n",
    "\n",
    "interval = z * sqrt( (error * (1 - error)) / n)\n",
    "\n",
    "interval = z * sqrt( (error * (1 - error)) / n)\n",
    "\n",
    "In the case of classification accuracy, the radius of the interval can be calculated as:\n",
    "\n",
    "interval = z * sqrt( (accuracy * (1 - accuracy)) / n)\n",
    "\n",
    "interval = z * sqrt( (accuracy * (1 - accuracy)) / n)\n",
    "\n",
    "Where interval is the radius of the confidence interval, error and accuracy are classification error and classification accuracy respectively, n is the size of the sample, sqrt is the square root function, and z is the number of standard deviations from the Gaussian distribution. Technically, this is called the Binomial proportion confidence interval.\n",
    "\n",
    "Commonly used number of standard deviations from the Gaussian distribution and their corresponding significance level are as follows:\n",
    "\n",
    "1.64 (90%)\n",
    "1.96 (95%)\n",
    "2.33 (98%)\n",
    "2.58 (99%)\n",
    "Consider a model with an error of 20%, or 0.2 (error = 0.2), on a validation dataset with 50 examples (n = 50). We can calculate the 95% confidence interval (z = 1.96) as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.111\n"
     ]
    }
   ],
   "source": [
    "# binomial confidence interval\n",
    "from math import sqrt\n",
    "interval = 1.96 * sqrt((0.2 * (1 - 0.2)) / 50)\n",
    "print('%.3f' % interval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the example, we see the calculated radius of the confidence interval calculated and printed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then make claims such as:\n",
    "\n",
    "- The classification error of the model is 20% +/- 11%\n",
    "- The true classification error of the model is likely between 9% and 31%.\n",
    "\n",
    "We can see the impact that the sample size has on the precision of the estimate in terms of the radius of the confidence interval.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.078\n"
     ]
    }
   ],
   "source": [
    "# binomial confidence interval\n",
    "interval = 1.96 * sqrt( (0.2 * (1 - 0.2)) / 100)\n",
    "print('%.3f' % interval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the example shows that the confidence interval drops to about 7%, increasing the precision of the estimate of the models skill"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, if we repeated this experiment over and over, each time drawing a new sample S, containing […] new examples, we would find that for approximately 95% of these experiments, the calculated interval would contain the true error. For this reason, we call this interval the 95% confidence interval estimate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The proportion_confint() statsmodels function an implementation of the binomial proportion confidence interval.\n",
    "\n",
    "By default, it makes the Gaussian assumption for the Binomial distribution, although other more sophisticated variations on the calculation are supported. The function takes the count of successes (or failures), the total number of trials, and the significance level as arguments and returns the lower and upper bound of the confidence interval.\n",
    "\n",
    "The example below demonstrates this function in a hypothetical case where a model made 88 correct predictions out of a dataset with 100 instances and we are interested in the 95% confidence interval (provided to the function as a significance of 0.05)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lower=0.816, upper=0.944\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.stats.proportion import proportion_confint\n",
    "lower, upper = proportion_confint(88, 100, 0.05)\n",
    "print('lower=%.3f, upper=%.3f' % (lower, upper))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the example prints the lower and upper bounds on the model’s classification accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nonparametric Confidence Interval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often we do not know the distribution for a chosen performance measure. Alternately, we may not know the analytical way to calculate a confidence interval for a skill score.\n",
    "\n",
    "_The assumptions that underlie parametric confidence intervals are often violated. The predicted variable sometimes isn’t normally distributed, and even when it is, the variance of the normal distribution might not be equal at all levels of the predictor variable._\n",
    "\n",
    "— Page 326, Empirical Methods for Artificial Intelligence, 1995.\n",
    "\n",
    "In these cases, the bootstrap resampling method can be used as a nonparametric method for calculating confidence intervals, nominally called bootstrap confidence intervals.\n",
    "\n",
    "The bootstrap is a simulated Monte Carlo method where samples are drawn from a fixed finite dataset with replacement and a parameter is estimated on each sample. This procedure leads to a robust estimate of the true population parameter via sampling.\n",
    "\n",
    "We can demonstrate this with the following pseudocode.\n",
    "\n",
    "statistics = []\n",
    "for i in bootstraps:\n",
    "\tsample = select_sample_with_replacement(data)\n",
    "\tstat = calculate_statistic(sample)\n",
    "\tstatistics.append(stat)\n",
    "    \n",
    "The procedure can be used to estimate the skill of a predictive model by fitting the model on each sample and evaluating the skill of the model on those samples not included in the sample. The mean or median skill of the model can then be presented as an estimate of the model skill when evaluated on unseen data.\n",
    "\n",
    "Confidence intervals can be added to this estimate by selecting observations from the sample of skill scores at specific percentiles.\n",
    "\n",
    "Recall that a percentile is an observation value drawn from the sorted sample where a percentage of the observations in the sample fall. For example, the 70th percentile of a sample indicates that 70% of the samples fall below that value. The 50th percentile is the median or middle of the distribution.\n",
    "\n",
    "First, we must choose a significance level for the confidence level, such as 95%, represented as 5.0% (e.g. 100 – 95). Because the confidence interval is symmetric around the median, we must choose observations at the 2.5th percentile and the 97.5th percentiles to give the full range.\n",
    "\n",
    "We can make the calculation of the bootstrap confidence interval concrete with a worked example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s assume we have a dataset of 1,000 observations of values between 0.5 and 1.0 drawn from a uniform distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate dataset\n",
    "import numpy as np\n",
    "dataset = 0.5 + np.random.rand(1000) * 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will perform the bootstrap procedure 100 times and draw samples of 1,000 observations from the dataset with replacement. We will estimate the mean of the population as the statistic we will calculate on the bootstrap samples. This could just as easily be a model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50th percentile (median) = 0.748\n"
     ]
    }
   ],
   "source": [
    "# bootstrap\n",
    "scores = list()\n",
    "for _ in range(100):\n",
    "    # bootstrap sample\n",
    "    indices = np.random.randint(0, 1000, 1000)\n",
    "    sample = dataset[indices]\n",
    "    # calculate and store statistic\n",
    "    statistic = np.mean(sample)\n",
    "    scores.append(statistic)\n",
    "print('50th percentile (median) = %.3f' % np.median(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have a sample of bootstrap statistics, we can calculate the central tendency. We will use the median or 50th percentile as we do not assume any distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "median=0.748\n"
     ]
    }
   ],
   "source": [
    "print('median=%.3f' % np.median(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then calculate the confidence interval as the middle 95% of observed statistical values centered around the median."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate 95% confidence intervals (100 - alpha)\n",
    "alpha = 5.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, the desired lower percentile is calculated based on the chosen confidence interval. Then the observation at this percentile is retrieved from the sample of bootstrap statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5th percentile = 0.740\n",
      "97.5th percentile = 0.757\n"
     ]
    }
   ],
   "source": [
    "# calculate lower percentile (e.g. 2.5)\n",
    "lower_p = alpha / 2.0\n",
    "# retrieve observation at lower percentile\n",
    "lower = max(0.0, np.percentile(scores, lower_p))\n",
    "print('%.1fth percentile = %.3f' % (lower_p, lower))\n",
    "\n",
    "# calculate upper percentile (e.g. 97.5)\n",
    "upper_p = (100 - alpha) + (alpha / 2.0)\n",
    "# retrieve observation at upper percentile\n",
    "upper = min(1.0, np.percentile(scores, upper_p))\n",
    "print('%.1fth percentile = %.3f' % (upper_p, upper))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then use these observations to make a claim about the sample distribution, such as:\n",
    "\n",
    "There is a 95% likelihood that the range 0.741 to 0.757 covers the true statistic mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  More Resources:\n",
    "- https://www.youtube.com/watch?v=TqOeMYtOc1w - Josh Starmer\n",
    "- https://towardsdatascience.com/how-to-add-confidence-intervals-to-any-model-7bbb9f80fd9c - add confidence interval to any model (10 lines of code)\n",
    "- https://towardsdatascience.com/a-very-friendly-introduction-to-confidence-intervals-9add126e714 - intro to confidence interval"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
